{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description â€“ Twitter US Airline Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is from a dataset from Kaggle.\n",
    "\n",
    "Link to the Kaggle project site: https://www.kaggle.com/crowdflower/twitter-airline-sentiment The dataset has to be downloaded from the above Kaggle website.\n",
    "\n",
    "The dataset has the following columns:\n",
    "    \n",
    "- tweet_id                             \n",
    "- airline_sentiment                    \n",
    "- airline_sentiment_confidence         \n",
    "- negativereason                       \n",
    "- negativereason_confidence            \n",
    "- airline                              \n",
    "- airline_sentiment_gold               \n",
    "- name\n",
    "- negativereason_gold\n",
    "- retweet_count\n",
    "- text\n",
    "- tweet_coord\n",
    "- tweet_created\n",
    "- tweet_location\n",
    "- user_timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the techniques learnt as a part of the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic understanding of text pre-processing.\n",
    "- What to do after text pre-processing:\n",
    "    - Bag of words\n",
    "    - Tf-idf\n",
    "- Build the classification model.\n",
    "- Evaluate the Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "pd.set_option('display.max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME=\"Tweets.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data into pandas dataframe\n",
    "data = pd.read_csv(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "  tweet_coord              tweet_created tweet_location  \\\n",
       "0         NaN  2015-02-24 11:35:52 -0800            NaN   \n",
       "1         NaN  2015-02-24 11:15:59 -0800            NaN   \n",
       "2         NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
       "3         NaN  2015-02-24 11:15:36 -0800            NaN   \n",
       "4         NaN  2015-02-24 11:14:45 -0800            NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Eastern Time (US & Canada)  \n",
       "1  Pacific Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3  Pacific Time (US & Canada)  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      14640 non-null  int64  \n",
      " 1   airline_sentiment             14640 non-null  object \n",
      " 2   airline_sentiment_confidence  14640 non-null  float64\n",
      " 3   negativereason                9178 non-null   object \n",
      " 4   negativereason_confidence     10522 non-null  float64\n",
      " 5   airline                       14640 non-null  object \n",
      " 6   airline_sentiment_gold        40 non-null     object \n",
      " 7   name                          14640 non-null  object \n",
      " 8   negativereason_gold           32 non-null     object \n",
      " 9   retweet_count                 14640 non-null  int64  \n",
      " 10  text                          14640 non-null  object \n",
      " 11  tweet_coord                   1019 non-null   object \n",
      " 12  tweet_created                 14640 non-null  object \n",
      " 13  tweet_location                9907 non-null   object \n",
      " 14  user_timezone                 9820 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "#Determine if there are any missing values. The number of variables, whether numeric and/or string and the total number of rows.\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['tweet_id','airline_sentiment_confidence','negativereason','negativereason_confidence','airline', 'airline_sentiment_gold','name','negativereason_gold','retweet_count','tweet_coord','tweet_created','tweet_location','user_timezone'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 2)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                                                             text  \n",
       "0                                                                                             @VirginAmerica What @dhepburn said.  \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.  \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!  \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse  \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing:\n",
    "\n",
    "- Remove html tags.\n",
    "- Replace contractions in string. (e.g. replace I'm --> I am) and so on.\\\n",
    "- Remove numbers.\n",
    "- Tokenization\n",
    "- To remove Stopwords.\n",
    "- Lemmatized data\n",
    "\n",
    "We have used NLTK library to tokenize words , remove stopwords and lemmatize the remaining words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing: Data preparation. (20 Marks)\n",
    "- a. Html tag removal.\n",
    "- b. Tokenization.\n",
    "- c. Remove the numbers.\n",
    "- d. Removal of Special Characters and Punctuations.\n",
    "- e. Conversion to lowercase.\n",
    "- f. Lemmatize or stemming.\n",
    "- g. Join the words in the list to convert back to text string in the dataframe. (So that each row\n",
    "contains the data in text format.)\n",
    "- h. Print first 5 rows of data after pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "        \n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    text = strip_html(text)\n",
    "    words = tokenize_text(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = replace_contractions(text)\n",
    "    \n",
    "    \n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = lemmatize_list(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kareemstreek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kareemstreek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords   \n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "customlist = ['not', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn',\n",
    "        \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "        \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "stopwords = list(set(stopwords) - set(customlist))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                   virginamerica dhepburn say\n",
       "1                                          virginamerica plus add commercials experience tacky\n",
       "2                                      virginamerica nt today must mean need take another trip\n",
       "3    virginamerica really aggressive blast obnoxious entertainment guests face little recourse\n",
       "4                                                           virginamerica really big bad thing\n",
       "dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.head(5).apply(lambda row: normalize(row['text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization: (10 Marks)\n",
    "- a. Use CountVectorizer.\n",
    "- b. Use TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15051\n"
     ]
    }
   ],
   "source": [
    "# Use CountVectorizer object to create a matrix will all the words in every tweet\n",
    "# For this analysis will use the default parameters for CountVectorizer\n",
    "tweet_transformer = CountVectorizer().fit(data['text'])\n",
    "\n",
    "# Print total number of vocabulary words\n",
    "print(len(tweet_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2054)\t1\n",
      "  (0, 2263)\t1\n",
      "  (0, 3070)\t1\n",
      "  (0, 5455)\t1\n",
      "  (0, 5740)\t1\n",
      "  (0, 6733)\t1\n",
      "  (0, 6868)\t1\n",
      "  (0, 7381)\t1\n",
      "  (0, 7685)\t1\n",
      "  (0, 8392)\t1\n",
      "  (0, 9726)\t1\n",
      "  (0, 11020)\t1\n",
      "  (0, 11078)\t1\n",
      "  (0, 13167)\t1\n",
      "  (0, 13326)\t1\n",
      "  (0, 14273)\t1\n",
      "  (0, 14953)\t1\n",
      "(1, 15051)\n"
     ]
    }
   ],
   "source": [
    "# Check an example in detail and take the 4th tweet in the dataset and see its vector representation\n",
    "# For reference this is the text of the tweet: 'realli aggress blast obnoxi entertain guest face littl recours'\n",
    "tweet_3 = data['text'][3]\n",
    "\n",
    "vector_3 = tweet_transformer.transform([tweet_3])\n",
    "print(vector_3)\n",
    "print(vector_3.shape)\n",
    "\n",
    "# There are 9 unique words in this message and the second number (e.g. 124) will allow one to see what word that is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Matrix:  (14640, 15051)\n",
      "Amount of Non-Zero occurences:  234281\n"
     ]
    }
   ],
   "source": [
    "# Apply the transformer in the entire tweets series\n",
    "tweet_bag_of_words = tweet_transformer.transform(data['text'])\n",
    "# Check the shape and number of non-zero ocurrences\n",
    "print('Shape of Matrix: ', tweet_bag_of_words.shape)\n",
    "print('Amount of Non-Zero occurences: ', tweet_bag_of_words.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14640, 15051)\n"
     ]
    }
   ],
   "source": [
    "# Adjust the weights with TF-IDF\n",
    "# Each weight is calculated with the following formula:\n",
    "# Scenario: in one document with 100 words the word 'data' appears 5 times. There are 1,000 documents to classify and the word 'data' appears 90 times in all of them\n",
    "# TF = 5/100 = 0.05\n",
    "# IDF = log(1,000/90) = 1\n",
    "# Tf-idf weight = 0.05 * 1 = 0.05\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# Apply the transformer to the bag of words\n",
    "tweet_tfidf_transformer = TfidfTransformer().fit(tweet_bag_of_words)\n",
    "tweet_tfidf = tweet_tfidf_transformer.transform(tweet_bag_of_words)\n",
    "# Check the shape\n",
    "print(tweet_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;amp; they have little recourse</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment  \\\n",
       "0           neutral   \n",
       "1          positive   \n",
       "2           neutral   \n",
       "3          negative   \n",
       "4          negative   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "0                                                                                             @VirginAmerica What @dhepburn said.   \n",
       "1                                                        @VirginAmerica plus you've added commercials to the experience... tacky.   \n",
       "2                                                         @VirginAmerica I didn't today... Must mean I need to take another trip!   \n",
       "3  @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse   \n",
       "4                                                                         @VirginAmerica and it's a really big bad thing about it   \n",
       "\n",
       "   airline_sentiment_model  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        1  \n",
       "4                        1  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add new column for airline sentiment with binary outcome: 1 for negative comment 0 for not negative\n",
    "# Create dictionary to map\n",
    "sentiment_dictionary = {'negative': 1, 'neutral': 0, 'positive': 0}\n",
    "# Add new column mapping the dictionary\n",
    "data2['airline_sentiment_model'] = data2['airline_sentiment'].map(sentiment_dictionary)\n",
    "# Check first 5 rows\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10248 4392 14640\n"
     ]
    }
   ],
   "source": [
    "# Take X and y variables using the TF-IDF vectorization from the previous step\n",
    "X = tweet_tfidf\n",
    "y = data2['airline_sentiment_model']\n",
    "# Do a train test split - will choose to leave the default 30% of the data for testing\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.3)\n",
    "# Check size of each sample\n",
    "print(X_train.shape[0], X_test.shape[0], X_train.shape[0] + X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Multinomial Naives Bayes object\n",
    "tweet_sentiment_model = MultinomialNB()\n",
    "# Fit X_train and y_train to train the model\n",
    "tweet_sentiment_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: 1\n",
      "expected: 1\n"
     ]
    }
   ],
   "source": [
    "# Make one prediction \n",
    "print('predicted:', tweet_sentiment_model.predict(X_test)[0])\n",
    "print('expected:', y_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to predict X_test values\n",
    "predictions = tweet_sentiment_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 670  965]\n",
      " [  47 2710]]\n"
     ]
    }
   ],
   "source": [
    "# Print confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print (confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.41      0.57      1635\n",
      "           1       0.74      0.98      0.84      2757\n",
      "\n",
      "    accuracy                           0.77      4392\n",
      "   macro avg       0.84      0.70      0.71      4392\n",
      "weighted avg       0.81      0.77      0.74      4392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance will always be subjective to what metric is more relevant to the model's objective. \n",
    "Will go through a few key points: \n",
    "\n",
    "- 0 - tweet is not negative \n",
    "- 1 - tweet is negative \n",
    "\n",
    "\n",
    "Model is showing a promsing recall rate, or probability of detection, by classifying correctly 99% of all negative tweets.\n",
    "\n",
    "\n",
    "Precision indicates there is potential for improvement with about 72% of negative preditions to be correctly classified.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "text_classifier2 = KNeighborsClassifier(n_neighbors = 5)#no of neighbors is hpyer parameter\n",
    "text_classifier2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = text_classifier2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1200  435]\n",
      " [ 539 2218]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71      1635\n",
      "           1       0.84      0.80      0.82      2757\n",
      "\n",
      "    accuracy                           0.78      4392\n",
      "   macro avg       0.76      0.77      0.77      4392\n",
      "weighted avg       0.78      0.78      0.78      4392\n",
      "\n",
      "0.7782331511839709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,predictions2))\n",
    "print(classification_report(y_test,predictions2))\n",
    "print(accuracy_score(y_test, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8338215191229512"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model2 = LogisticRegression(max_iter=2000,multi_class='ovr')\n",
    "cross_val_score(model2,X_train,y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8029872513659251"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model3 = RandomForestClassifier()\n",
    "cross_val_score(model3,X_train,y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
